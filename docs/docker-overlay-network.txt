
https://blog.revolve.team/2017/04/25/deep-dive-into-docker-overlay-networks-part-1/
https://blog.revolve.team/2017/05/09/deep-dive-into-docker-overlay-networks-part-2/
https://blog.revolve.team/2017/08/20/deep-dive-3-into-docker-overlay-networks-part-3/

##########
# Consul #
##########

enable firewall:

aws ec2 authorize-security-group-ingress --group-id "sg-04602097ed29d4711" --protocol "tcp" --port "38142" --cidr "0.0.0.0/0" ## for ssh access
aws ec2 authorize-security-group-ingress --group-id "sg-04602097ed29d4711" --protocol "tcp" --port "8500" --cidr "0.0.0.0/0"  ## for ui
aws ec2 authorize-security-group-ingress --group-id "sg-04602097ed29d4711" --protocol "tcp" --port "2377" --cidr "0.0.0.0/0"  ## for cluster management communications
aws ec2 authorize-security-group-ingress --group-id "sg-04602097ed29d4711" --protocol "tcp" --port "7946" --cidr "0.0.0.0/0"  ## for communication among nodes
aws ec2 authorize-security-group-ingress --group-id "sg-04602097ed29d4711" --protocol "udp" --port "7946" --cidr "0.0.0.0/0"  ## for communication among nodes
aws ec2 authorize-security-group-ingress --group-id "sg-04602097ed29d4711" --protocol "udp" --port "4789" --cidr "0.0.0.0/0"  ## for overlay network traffic

sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
sudo yum -y install consul

sudo vim /usr/lib/systemd/system/consul.service

	ExecStart=/usr/bin/consul agent -config-dir=/etc/consul.d/ -server -dev -ui -client 0.0.0.0
	sudo systemctl daemon-reload
	sudo systemctl restart consul
	
http://18.202.34.174:8500/ui

##########
# Docker #
##########

sudo vim /usr/lib/systemd/system/docker.service

	ExecStart=/usr/bin/dockerd -H fd:// --cluster-store=consul://10.0.10.34:8500 --cluster-advertise=eth0:2377 --containerd=/run/containerd/containerd.sock $OPTIONS 	 $DOCKER_STORAGE_OPTIONS $DOCKER_ADD_RUNTIMES

sudo systemctl daemon-reload
sudo systemctl restart docker

#######################################
# ((1)) Overlay with Docker command
#######################################

----->>> on sinatra or redis host
sudo docker network create --driver overlay --subnet 192.168.0.0/24 demonet
sudo docker network ls

##############################
# Run container in overlay
##############################

aws iam attach-role-policy --role-name "AdminAWSrole" --policy-arn "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess"
aws iam attach-role-policy --role-name "RedisAWSrole" --policy-arn "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess"
aws iam attach-role-policy --role-name "SinatraAWSrole" --policy-arn "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess"

login_pwd=$(aws ecr get-login-password --region "eu-west-1")
echo $"${login_pwd}" | sudo docker login --username 'AWS' --password-stdin "955230900736.dkr.ecr.eu-west-1.amazonaws.com" 
sudo docker run --name demo1 -d --net=demonet "955230900736.dkr.ecr.eu-west-1.amazonaws.com/maxmin13/centos8:v1"  sleep 3600 

sudo docker inspect demo1
sudo docker network inspect demonet

############################
# Network analysis
############################

-----------------------
-- DOCKER NAMESPACES --
-----------------------

   sudo docker network inspect demonet -f {{.Id}} ---->>> the string should contain the overlay namespace ID at the beginning
   sudo docker inspect demo1 -f {{.NetworkSettings.SandboxKey}} ---->>>> container namespace
   
   sudo ls -1 /var/run/docker/netns
   
   overlayNS='/var/run/docker/netns/1-b2bad74119'
   containerNS='/var/run/docker/netns/c8014be5f82c'
   
   ### or
   
   containerNS=$(sudo docker inspect demo1 -f {{.NetworkSettings.SandboxKey}}) 
   
   ######## NB any time a container are restarted the ns ids change ########

--------------------------
-- CONTAINER INTERFACES --
--------------------------

   sudo docker exec demo1 ip -details link show
   
   sudo docker exec demo1 ip addr show eth0
   sudo docker exec demo1 ip addr show eth1
   
   sudo docker exec demo1 ip --details link show eth0 --->>> type veth
   sudo docker exec demo1 ip --details link show eth1 --->>> type veth
   
   ## or 
   sudo nsenter --net=$containerNS ip link sh
   
------------------------
-- OVERLAY INTERFACES --
------------------------

   sudo nsenter --net=$overlayNS ip --details link sh
   
   --->> br0
   --->> veth0
   --->> vxlan0

--------------------
-- ROUTING CONFIG --
--------------------

   sudo docker exec demo1 ip route show
  
---------------------------
-- PEERED veth INTERFACE --
---------------------------

   sudo nsenter --net=$containerNS ip --details link show 
   sudo nsenter --net=$containerNS ethtool -S eth0  ---->>>>> peer_ifindex: 7 
   sudo nsenter --net=$containerNS ethtool -S eth1  ---->>>>> peer_ifindex: 10

   ---->>> get the index and first check in the interfaces in the host:
   ip -details link show
   
   ---->>> eth1 <<<---- 
   ---->>> is peered with a veth interface in the host, which is attached to a bridge docker_gwbridge 
   
   ip add show docker_gwbridge 
   bridge link
    
   sudo docker network ls
   sudo docker network inspect docker_gwbridge 
   
   ----->>>>>>  icc=false, ip_masquerade=true
   
   --->>>> iproute2: A bridge is a piece of software used to unite two or more network segments. 
   --->>>> iproute2: A bridge behaves like a virtual network switch, working transparently (the other machines do not need to know about its existence). 
   --->>>> iproute2: Any real devices (e.g. eth0) and virtual devices (e.g. tap0) can be connected to it.
   
   ###### why there is NAT in the picture ??????
   ###### forward through host eth0

   ---->>>>> The interface peered with container eth0 is not in the host.
   ---->>>>> check in the overlay namespace
   sudo nsenter --net=$overlayNS ip --details link show

   ---->>>> 'veth0' <<<---- 
   ---->>>> attached to 'br0', attached to 'vxlan0'
   
   sudo nsenter --net=$containerNS ethtool -S eth0

-------------
-- TCPDUMP --
-------------

sudo nsenter --net=$containerNS tcpdump -i eth0

## or (container must be running)

ctn_ns_path=$(sudo docker inspect --format="{{ .NetworkSettings.SandboxKey}}" demo1)
ctn_ns=${ctn_ns_path##*/}
sudo ln -sf $ctn_ns_path /var/run/netns/$ctn_ns
sudo ip netns exec $ctn_ns ip link show













##############################################
# https://github.com/lbernail/dockeroverlays
##############################################

# create an network namespace 

sudo ip netns add overns
ip netns list

# create a bridge in this namespace, give it an IP address and bring the interface up

sudo ip netns exec overns ip link add dev br0 type bridge
sudo ip netns exec overns ip addr add dev br0 192.168.0.1/24
sudo ip netns exec overns ip link set br0 up
sudo ip netns exec overns ip link show


# create a VXLAN interface and attach it to the bridge.
# tunnel traffic on the standard VXLAN port.
# he proxy option allows the vxlan interface to answer ARP queries.
# we did not create the VXLAN interface inside the namespace but on the host and then moved it to the namespace. 
# This is necessary so the VXLAN interface can keep a link with our main host interface and send traffic over the network. 
# If we had created the interface inside the namespace (like we did for br0) we would not have been able to send traffic outside the namespace.

sudo ip link add dev vxlan1 type vxlan id 42 proxy learning dstport 4789
sudo ip link set vxlan1 netns overns
sudo ip netns exec overns ip link set vxlan1 master br0
sudo ip netns exec overns ip link set vxlan1 up	
sudo ip netns exec overns ip link show

	

# path of the network namespace for this container
ctn_ns_path=$(sudo docker inspect --format="{{ .NetworkSettings.SandboxKey}}" demo)

# the container has no network connectivity because of the --net=none option.
# create a veth and move one of its endpoints (veth1) to our overlay network namespace, attach it to the bridge and bring it up.
#  MTU of 1450 which is necessary due to the overhead added by the VXLAN header.

sudo ip link add dev veth1 mtu 1450 type veth peer name veth2 mtu 1450 ## this creates veth1 and veth2 devices
sudo ip link set dev veth1 netns overns
sudo ip netns exec overns ip link set veth1 master br0
sudo ip netns exec overns ip link set veth1 up
sudo ip netns exec overns ip link show

# configure veth2, send it to our container network namespace and configure it with a MAC address (02:42:c0:a8:00:02) and an IP address (192.168.0.2).
# We have to do the same on the other hosts with different MAC and IP addresses (02:42:c0:a8:00:03 and 192.168.0.3).

ctn_ns=${ctn_ns_path##*/}

# The symbolic link in /var/run/netns is required so we can use the native ip netns commands (to move the interface to the container network namespace). 

sudo ln -sf $ctn_ns_path /var/run/netns/$ctn_ns
sudo ip link set dev veth2 netns $ctn_ns
sudo ip netns exec overns ip link show
sudo ip netns exec $ctn_ns ip link show

# We used the same addressing schem as Docker: 
# the last 4 bytes of the MAC address match the IP address of the container and the second one is the VXLAN id.

sudo ip netns exec $ctn_ns ip link set dev veth2 name eth0 address 02:42:c0:a8:00:02
sudo ip netns exec $ctn_ns ip addr add dev eth0 192.168.0.2/24
sudo ip netns exec $ctn_ns ip link set dev eth0 up

sudo rm /var/run/netns/$ctn_ns

# populating the ARP and FDB entries in the overlay namespace:
sudo ip netns exec overns ip neighbor add 192.168.0.3 lladdr 02:42:c0:a8:00:03 dev vxlan1

# configures the forwarding table by telling it the MAC address is accessible using the VXLAN interface, with VXLAN id 42 and on host 10.0.10.30
sudo ip netns exec overns bridge fdb add 02:42:c0:a8:00:03 dev vxlan1 self dst 10.0.10.30 vni 42 port 4789






#
# test connectivity
#

sudo docker exec -it demo ping 192.168.0.3

#
# check ARP config in container and overlay ns
#

sudo docker exec demo ip neighbor show
sudo ip netns exec overns ip neighbor show

# We can verify that our command is generating an ARP query by running tcpdump in the overlay namespace rerun the ping command from another terminal
sudo ip netns exec overns tcpdump -i br0





